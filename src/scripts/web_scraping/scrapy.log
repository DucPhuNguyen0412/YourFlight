INFO:scrapy.utils.log:Scrapy 2.9.0 started (bot: scrapybot)
INFO:scrapy.utils.log:Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-13.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings:
{'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 '
               'Safari/605.1.15'}
WARNING:py.warnings:/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

DEBUG:scrapy.utils.log:Using reactor: twisted.internet.selectreactor.SelectReactor
INFO:scrapy.extensions.telnet:Telnet Password: 2e0910f2cfa46f5c
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
CRITICAL:twisted:Unhandled error in Deferred:
CRITICAL:twisted:
Traceback (most recent call last):
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/crawler.py", line 128, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/crawler.py", line 140, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/src/scripts/web_scraping/amazon_web_scraping.py", line 76, in from_crawler
    spider = super(AmazonSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/spiders/__init__.py", line 53, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/src/scripts/web_scraping/amazon_web_scraping.py", line 45, in __init__
    self.models = models.split(',')
AttributeError: 'list' object has no attribute 'split'
INFO:scrapy.utils.log:Scrapy 2.9.0 started (bot: scrapybot)
INFO:scrapy.utils.log:Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-13.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings:
{'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 '
               'Safari/605.1.15'}
WARNING:py.warnings:/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

DEBUG:scrapy.utils.log:Using reactor: twisted.internet.selectreactor.SelectReactor
INFO:scrapy.extensions.telnet:Telnet Password: 28204855eb2902e8
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
CRITICAL:twisted:Unhandled error in Deferred:
CRITICAL:twisted:
Traceback (most recent call last):
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/twisted/internet/defer.py", line 1697, in _inlineCallbacks
    result = context.run(gen.send, result)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/crawler.py", line 128, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/crawler.py", line 140, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/src/scripts/web_scraping/amazon_web_scraping.py", line 82, in from_crawler
    spider = super(AmazonSpider, cls).from_crawler(crawler, *args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/spiders/__init__.py", line 53, in from_crawler
    spider = cls(*args, **kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/src/scripts/web_scraping/amazon_web_scraping.py", line 51, in __init__
    self.models = models.split(',')
AttributeError: 'list' object has no attribute 'split'
INFO:scrapy.utils.log:Scrapy 2.9.0 started (bot: scrapybot)
INFO:scrapy.utils.log:Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-13.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings:
{'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 '
               'Safari/605.1.15'}
WARNING:py.warnings:/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

DEBUG:scrapy.utils.log:Using reactor: twisted.internet.selectreactor.SelectReactor
INFO:scrapy.extensions.telnet:Telnet Password: 53c6ec6050bdd1b5
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023
DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.amazon.com/s?k=table> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function validate_ascii_metadata at 0x10425bb50>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function sse_md5 at 0x10425af80>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function convert_body_to_file_like_object at 0x1042744c0>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function validate_bucket_name at 0x10425aef0>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function remove_bucket_from_url_paths_from_model at 0x104274d30>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <bound method S3RegionRedirectorv2.annotate_request_context of <botocore.utils.S3RegionRedirectorv2 object at 0x1141aa9e0>>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function generate_idempotent_uuid at 0x10425ad40>
DEBUG:botocore.hooks:Event before-endpoint-resolution.s3: calling handler <function customize_endpoint_resolver_builtins at 0x104274ee0>
DEBUG:botocore.hooks:Event before-endpoint-resolution.s3: calling handler <bound method S3RegionRedirectorv2.redirect_from_cache of <botocore.utils.S3RegionRedirectorv2 object at 0x1141aa9e0>>
DEBUG:botocore.regions:Calling endpoint provider with parameters: {'Bucket': 'mybucket', 'Region': 'us-east-1', 'UseFIPS': False, 'UseDualStack': False, 'ForcePathStyle': False, 'Accelerate': False, 'UseGlobalEndpoint': True, 'DisableMultiRegionAccessPoints': False, 'UseArnRegion': True}
DEBUG:botocore.regions:Endpoint provider result: https://mybucket.s3.amazonaws.com
DEBUG:botocore.regions:Selecting from endpoint provider's list of auth schemes: "sigv4". User selected auth scheme is: "None"
DEBUG:botocore.regions:Selected auth type "v4" as "v4" with signing context params: {'region': 'us-east-1', 'signing_name': 's3', 'disableDoubleEncoding': True}
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function conditionally_calculate_md5 at 0x1041091b0>
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function add_expect_header at 0x10425b250>
DEBUG:botocore.handlers:Adding expect 100 continue header to request.
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function add_recursion_detection_header at 0x10425a9e0>
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function inject_api_version_header_if_needed at 0x1042745e0>
DEBUG:botocore.endpoint:Making request for OperationModel(name=PutObject) with params: {'url_path': '/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'query_string': {}, 'method': 'PUT', 'headers': {'User-Agent': 'Boto3/1.28.1 md/Botocore#1.31.1 ua/2.0 os/macos#22.1.0 md/arch#x86_64 lang/python#3.10.11 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.31.1', 'Content-MD5': 'rhltgysARsLrJeA6biw16A==', 'Expect': '100-continue'}, 'body': <_io.BytesIO object at 0x118838720>, 'auth_path': '/mybucket/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'url': 'https://mybucket.s3.amazonaws.com/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x11410c280>, 'has_streaming_input': True, 'auth_type': 'v4', 's3_redirect': {'redirected': False, 'bucket': 'mybucket', 'params': {'Bucket': 'mybucket', 'Body': <_io.BytesIO object at 0x118838720>, 'Key': 'data/raw/amazon/2023-07-11_amazon_raw_data.csv'}}, 'signing': {'region': 'us-east-1', 'signing_name': 's3', 'disableDoubleEncoding': True}}}
DEBUG:botocore.hooks:Event request-created.s3.PutObject: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x114038850>>
DEBUG:botocore.hooks:Event choose-signer.s3.PutObject: calling handler <bound method ClientCreator._default_s3_presign_to_sigv2 of <botocore.client.ClientCreator object at 0x113832a70>>
DEBUG:botocore.hooks:Event choose-signer.s3.PutObject: calling handler <function set_operation_specific_signer at 0x10425ac20>
DEBUG:botocore.hooks:Event before-sign.s3.PutObject: calling handler <function remove_arn_from_signing_path at 0x104274e50>
DEBUG:botocore.auth:Calculating signature using v4 auth.
DEBUG:botocore.auth:CanonicalRequest:
PUT
/data/raw/amazon/2023-07-11_amazon_raw_data.csv

content-md5:rhltgysARsLrJeA6biw16A==
host:mybucket.s3.amazonaws.com
x-amz-content-sha256:UNSIGNED-PAYLOAD
x-amz-date:20230711T154915Z

content-md5;host;x-amz-content-sha256;x-amz-date
UNSIGNED-PAYLOAD
DEBUG:botocore.auth:StringToSign:
AWS4-HMAC-SHA256
20230711T154915Z
20230711/us-east-1/s3/aws4_request
0dc8bf1be25de87ce56438bd40ab97b0a3fa17de8a3fc06bc228d365f7370b3f
DEBUG:botocore.auth:Signature:
bacb7297f60ac8ea9bd7600af22bf5fba19c617ea621b9f33e89c2e767ec4d6f
DEBUG:botocore.hooks:Event request-created.s3.PutObject: calling handler <function add_retry_headers at 0x104274ca0>
DEBUG:botocore.endpoint:Sending http request: <AWSPreparedRequest stream_output=False, method=PUT, url=https://mybucket.s3.amazonaws.com/data/raw/amazon/2023-07-11_amazon_raw_data.csv, headers={'User-Agent': b'Boto3/1.28.1 md/Botocore#1.31.1 ua/2.0 os/macos#22.1.0 md/arch#x86_64 lang/python#3.10.11 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.31.1', 'Content-MD5': b'rhltgysARsLrJeA6biw16A==', 'Expect': b'100-continue', 'X-Amz-Date': b'20230711T154915Z', 'X-Amz-Content-SHA256': b'UNSIGNED-PAYLOAD', 'Authorization': b'AWS4-HMAC-SHA256 Credential=AKIAV6IEX2Z6QRBLWLKQ/20230711/us-east-1/s3/aws4_request, SignedHeaders=content-md5;host;x-amz-content-sha256;x-amz-date, Signature=bacb7297f60ac8ea9bd7600af22bf5fba19c617ea621b9f33e89c2e767ec4d6f', 'amz-sdk-invocation-id': b'dde73e7d-3dd3-428f-8c6a-5bd0b778d441', 'amz-sdk-request': b'attempt=1', 'Content-Length': '46648'}>
DEBUG:botocore.httpsession:Certificate path: /Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/certifi/cacert.pem
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): mybucket.s3.amazonaws.com:443
DEBUG:botocore.awsrequest:Waiting for 100 Continue response.
DEBUG:botocore.awsrequest:Received a non 100 Continue response from the server, NOT sending request body.
DEBUG:urllib3.connectionpool:https://mybucket.s3.amazonaws.com:443 "PUT /data/raw/amazon/2023-07-11_amazon_raw_data.csv HTTP/1.1" 403 None
DEBUG:botocore.parsers:Response headers: {'x-amz-request-id': '2V2E1K23HZAAT2ER', 'x-amz-id-2': 'mPBtgN6woSRwH0DMzkZqUDR5RA6WcMG2e/zBmwaEnv2dKJs1qu4LxkO/udqc9YwU7GNfnhvCK+yNqB3lf8reCAlcAsUtHg3H', 'Content-Type': 'application/xml', 'Transfer-Encoding': 'chunked', 'Date': 'Tue, 11 Jul 2023 15:49:16 GMT', 'Server': 'AmazonS3', 'Connection': 'close'}
DEBUG:botocore.parsers:Response body:
b'<?xml version="1.0" encoding="UTF-8"?>\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>2V2E1K23HZAAT2ER</RequestId><HostId>mPBtgN6woSRwH0DMzkZqUDR5RA6WcMG2e/zBmwaEnv2dKJs1qu4LxkO/udqc9YwU7GNfnhvCK+yNqB3lf8reCAlcAsUtHg3H</HostId></Error>'
DEBUG:botocore.hooks:Event needs-retry.s3.PutObject: calling handler <botocore.retryhandler.RetryHandler object at 0x1141aa920>
DEBUG:botocore.retryhandler:No retry needed.
DEBUG:botocore.hooks:Event needs-retry.s3.PutObject: calling handler <bound method S3RegionRedirectorv2.redirect_from_error of <botocore.utils.S3RegionRedirectorv2 object at 0x1141aa9e0>>
ERROR:scrapy.utils.signal:Error caught on signal handler: <bound method AmazonSpider.spider_closed of <AmazonSpider 'amazon_spider' at 0x1143506d0>>
Traceback (most recent call last):
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/defer.py", line 315, in maybeDeferred_coro
    result = f(*args, **kw)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/src/scripts/web_scraping/amazon_web_scraping.py", line 97, in spider_closed
    s3.put_object(Bucket=self.bucket, Body=csv_buffer.getvalue(), Key=csv_key)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/botocore/client.py", line 534, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/botocore/client.py", line 976, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 306,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 185247,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.48479,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 7, 11, 15, 49, 15, 195492),
 'httpcompression/response_bytes': 1208866,
 'httpcompression/response_count': 1,
 'log_count/DEBUG': 41,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'memusage/max': 114548736,
 'memusage/startup': 114548736,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 7, 11, 15, 49, 11, 710702)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 2.9.0 started (bot: scrapybot)
INFO:scrapy.utils.log:Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-13.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings:
{'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 '
               'Safari/605.1.15'}
WARNING:py.warnings:/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

DEBUG:scrapy.utils.log:Using reactor: twisted.internet.selectreactor.SelectReactor
INFO:scrapy.extensions.telnet:Telnet Password: c9a4848bb3cead89
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023
DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.amazon.com/s?k=table> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function validate_ascii_metadata at 0x110263b50>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function sse_md5 at 0x110262f80>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function convert_body_to_file_like_object at 0x11027c4c0>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function validate_bucket_name at 0x110262ef0>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function remove_bucket_from_url_paths_from_model at 0x11027cd30>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <bound method S3RegionRedirectorv2.annotate_request_context of <botocore.utils.S3RegionRedirectorv2 object at 0x1201b2650>>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function generate_idempotent_uuid at 0x110262d40>
DEBUG:botocore.hooks:Event before-endpoint-resolution.s3: calling handler <function customize_endpoint_resolver_builtins at 0x11027cee0>
DEBUG:botocore.hooks:Event before-endpoint-resolution.s3: calling handler <bound method S3RegionRedirectorv2.redirect_from_cache of <botocore.utils.S3RegionRedirectorv2 object at 0x1201b2650>>
DEBUG:botocore.regions:Calling endpoint provider with parameters: {'Bucket': 'mybucket', 'Region': 'us-east-1', 'UseFIPS': False, 'UseDualStack': False, 'ForcePathStyle': False, 'Accelerate': False, 'UseGlobalEndpoint': True, 'DisableMultiRegionAccessPoints': False, 'UseArnRegion': True}
DEBUG:botocore.regions:Endpoint provider result: https://mybucket.s3.amazonaws.com
DEBUG:botocore.regions:Selecting from endpoint provider's list of auth schemes: "sigv4". User selected auth scheme is: "None"
DEBUG:botocore.regions:Selected auth type "v4" as "v4" with signing context params: {'region': 'us-east-1', 'signing_name': 's3', 'disableDoubleEncoding': True}
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function conditionally_calculate_md5 at 0x1101111b0>
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function add_expect_header at 0x110263250>
DEBUG:botocore.handlers:Adding expect 100 continue header to request.
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function add_recursion_detection_header at 0x1102629e0>
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function inject_api_version_header_if_needed at 0x11027c5e0>
DEBUG:botocore.endpoint:Making request for OperationModel(name=PutObject) with params: {'url_path': '/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'query_string': {}, 'method': 'PUT', 'headers': {'User-Agent': 'Boto3/1.28.1 md/Botocore#1.31.1 ua/2.0 os/macos#22.1.0 md/arch#x86_64 lang/python#3.10.11 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.31.1', 'Content-MD5': '1axT19g3zwcTpIERFJaHaA==', 'Expect': '100-continue'}, 'body': <_io.BytesIO object at 0x12061c360>, 'auth_path': '/mybucket/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'url': 'https://mybucket.s3.amazonaws.com/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x120117a00>, 'has_streaming_input': True, 'auth_type': 'v4', 's3_redirect': {'redirected': False, 'bucket': 'mybucket', 'params': {'Bucket': 'mybucket', 'Body': <_io.BytesIO object at 0x12061c360>, 'Key': 'data/raw/amazon/2023-07-11_amazon_raw_data.csv'}}, 'signing': {'region': 'us-east-1', 'signing_name': 's3', 'disableDoubleEncoding': True}}}
DEBUG:botocore.hooks:Event request-created.s3.PutObject: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x1200404c0>>
DEBUG:botocore.hooks:Event choose-signer.s3.PutObject: calling handler <bound method ClientCreator._default_s3_presign_to_sigv2 of <botocore.client.ClientCreator object at 0x11f83a6e0>>
DEBUG:botocore.hooks:Event choose-signer.s3.PutObject: calling handler <function set_operation_specific_signer at 0x110262c20>
DEBUG:botocore.hooks:Event before-sign.s3.PutObject: calling handler <function remove_arn_from_signing_path at 0x11027ce50>
DEBUG:botocore.auth:Calculating signature using v4 auth.
DEBUG:botocore.auth:CanonicalRequest:
PUT
/data/raw/amazon/2023-07-11_amazon_raw_data.csv

content-md5:1axT19g3zwcTpIERFJaHaA==
host:mybucket.s3.amazonaws.com
x-amz-content-sha256:UNSIGNED-PAYLOAD
x-amz-date:20230711T160505Z

content-md5;host;x-amz-content-sha256;x-amz-date
UNSIGNED-PAYLOAD
DEBUG:botocore.auth:StringToSign:
AWS4-HMAC-SHA256
20230711T160505Z
20230711/us-east-1/s3/aws4_request
895042156e7f7452c00ed9b112b62d66829168bc279e9d0140fc53172819a6b0
DEBUG:botocore.auth:Signature:
68fc367580c6583b851cb2a01589cd3a9fa58ca51a2e1f7d8437e2d8227c99e6
DEBUG:botocore.hooks:Event request-created.s3.PutObject: calling handler <function add_retry_headers at 0x11027cca0>
DEBUG:botocore.endpoint:Sending http request: <AWSPreparedRequest stream_output=False, method=PUT, url=https://mybucket.s3.amazonaws.com/data/raw/amazon/2023-07-11_amazon_raw_data.csv, headers={'User-Agent': b'Boto3/1.28.1 md/Botocore#1.31.1 ua/2.0 os/macos#22.1.0 md/arch#x86_64 lang/python#3.10.11 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.31.1', 'Content-MD5': b'1axT19g3zwcTpIERFJaHaA==', 'Expect': b'100-continue', 'X-Amz-Date': b'20230711T160505Z', 'X-Amz-Content-SHA256': b'UNSIGNED-PAYLOAD', 'Authorization': b'AWS4-HMAC-SHA256 Credential=AKIAV6IEX2Z6QRBLWLKQ/20230711/us-east-1/s3/aws4_request, SignedHeaders=content-md5;host;x-amz-content-sha256;x-amz-date, Signature=68fc367580c6583b851cb2a01589cd3a9fa58ca51a2e1f7d8437e2d8227c99e6', 'amz-sdk-invocation-id': b'6cd385f5-a71b-4d32-85b7-c79236fb2876', 'amz-sdk-request': b'attempt=1', 'Content-Length': '45562'}>
DEBUG:botocore.httpsession:Certificate path: /Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/certifi/cacert.pem
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): mybucket.s3.amazonaws.com:443
DEBUG:botocore.awsrequest:Waiting for 100 Continue response.
DEBUG:botocore.awsrequest:Received a non 100 Continue response from the server, NOT sending request body.
DEBUG:urllib3.connectionpool:https://mybucket.s3.amazonaws.com:443 "PUT /data/raw/amazon/2023-07-11_amazon_raw_data.csv HTTP/1.1" 403 None
DEBUG:botocore.parsers:Response headers: {'x-amz-request-id': 'Z5B0TPMN3WRJ3NGZ', 'x-amz-id-2': 'qhUQV/v9LQTdoHktuc/KwwVYWqyu6Px/IXUIVAFND/oP26OICYCvClnGkOFuK8Vrr5NB0R4CQl9HhA/mxTcqayumLqlQTE01ZQOj5KtG07g=', 'Content-Type': 'application/xml', 'Transfer-Encoding': 'chunked', 'Date': 'Tue, 11 Jul 2023 16:05:06 GMT', 'Server': 'AmazonS3', 'Connection': 'close'}
DEBUG:botocore.parsers:Response body:
b'<?xml version="1.0" encoding="UTF-8"?>\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>Z5B0TPMN3WRJ3NGZ</RequestId><HostId>qhUQV/v9LQTdoHktuc/KwwVYWqyu6Px/IXUIVAFND/oP26OICYCvClnGkOFuK8Vrr5NB0R4CQl9HhA/mxTcqayumLqlQTE01ZQOj5KtG07g=</HostId></Error>'
DEBUG:botocore.hooks:Event needs-retry.s3.PutObject: calling handler <botocore.retryhandler.RetryHandler object at 0x1201b2590>
DEBUG:botocore.retryhandler:No retry needed.
DEBUG:botocore.hooks:Event needs-retry.s3.PutObject: calling handler <bound method S3RegionRedirectorv2.redirect_from_error of <botocore.utils.S3RegionRedirectorv2 object at 0x1201b2650>>
ERROR:scrapy.utils.signal:Error caught on signal handler: <bound method AmazonSpider.spider_closed of <AmazonSpider 'amazon_spider' at 0x12035c340>>
Traceback (most recent call last):
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/defer.py", line 315, in maybeDeferred_coro
    result = f(*args, **kw)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/pydispatch/robustapply.py", line 55, in robustApply
    return receiver(*arguments, **named)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/src/scripts/web_scraping/amazon_web_scraping.py", line 97, in spider_closed
    s3.put_object(Bucket=self.bucket, Body=csv_buffer.getvalue(), Key=csv_key)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/botocore/client.py", line 534, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File "/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/botocore/client.py", line 976, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 306,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 174434,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.869497,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 7, 11, 16, 5, 5, 688257),
 'httpcompression/response_bytes': 1113666,
 'httpcompression/response_count': 1,
 'log_count/DEBUG': 41,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'memusage/max': 114515968,
 'memusage/startup': 114515968,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 7, 11, 16, 5, 1, 818760)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 2.9.0 started (bot: scrapybot)
INFO:scrapy.utils.log:Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-13.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings:
{'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
               'AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 '
               'Safari/605.1.15'}
WARNING:py.warnings:/Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/scrapy/utils/request.py:232: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.

It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.

See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.
  return cls(crawler)

DEBUG:scrapy.utils.log:Using reactor: twisted.internet.selectreactor.SelectReactor
INFO:scrapy.extensions.telnet:Telnet Password: 314ee9b4a0b25225
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023
DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.amazon.com/s?k=table> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function validate_ascii_metadata at 0x110ea3b50>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function sse_md5 at 0x110ea2f80>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function convert_body_to_file_like_object at 0x110ebc4c0>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function validate_bucket_name at 0x110ea2ef0>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function remove_bucket_from_url_paths_from_model at 0x110ebcd30>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <bound method S3RegionRedirectorv2.annotate_request_context of <botocore.utils.S3RegionRedirectorv2 object at 0x120d3a950>>
DEBUG:botocore.hooks:Event before-parameter-build.s3.PutObject: calling handler <function generate_idempotent_uuid at 0x110ea2d40>
DEBUG:botocore.hooks:Event before-endpoint-resolution.s3: calling handler <function customize_endpoint_resolver_builtins at 0x110ebcee0>
DEBUG:botocore.hooks:Event before-endpoint-resolution.s3: calling handler <bound method S3RegionRedirectorv2.redirect_from_cache of <botocore.utils.S3RegionRedirectorv2 object at 0x120d3a950>>
DEBUG:botocore.regions:Calling endpoint provider with parameters: {'Bucket': 'bestpricenphu', 'Region': 'us-east-1', 'UseFIPS': False, 'UseDualStack': False, 'ForcePathStyle': False, 'Accelerate': False, 'UseGlobalEndpoint': True, 'DisableMultiRegionAccessPoints': False, 'UseArnRegion': True}
DEBUG:botocore.regions:Endpoint provider result: https://bestpricenphu.s3.amazonaws.com
DEBUG:botocore.regions:Selecting from endpoint provider's list of auth schemes: "sigv4". User selected auth scheme is: "None"
DEBUG:botocore.regions:Selected auth type "v4" as "v4" with signing context params: {'region': 'us-east-1', 'signing_name': 's3', 'disableDoubleEncoding': True}
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function conditionally_calculate_md5 at 0x110d491b0>
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function add_expect_header at 0x110ea3250>
DEBUG:botocore.handlers:Adding expect 100 continue header to request.
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function add_recursion_detection_header at 0x110ea29e0>
DEBUG:botocore.hooks:Event before-call.s3.PutObject: calling handler <function inject_api_version_header_if_needed at 0x110ebc5e0>
DEBUG:botocore.endpoint:Making request for OperationModel(name=PutObject) with params: {'url_path': '/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'query_string': {}, 'method': 'PUT', 'headers': {'User-Agent': 'Boto3/1.28.1 md/Botocore#1.31.1 ua/2.0 os/macos#22.1.0 md/arch#x86_64 lang/python#3.10.11 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.31.1', 'Content-MD5': 'YmDjjwY3eVhcUwc9vZT0hg==', 'Expect': '100-continue'}, 'body': <_io.BytesIO object at 0x1259d4c20>, 'auth_path': '/bestpricenphu/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'url': 'https://bestpricenphu.s3.amazonaws.com/data/raw/amazon/2023-07-11_amazon_raw_data.csv', 'context': {'client_region': 'us-east-1', 'client_config': <botocore.config.Config object at 0x120c981f0>, 'has_streaming_input': True, 'auth_type': 'v4', 's3_redirect': {'redirected': False, 'bucket': 'bestpricenphu', 'params': {'Bucket': 'bestpricenphu', 'Body': <_io.BytesIO object at 0x1259d4c20>, 'Key': 'data/raw/amazon/2023-07-11_amazon_raw_data.csv'}}, 'signing': {'region': 'us-east-1', 'signing_name': 's3', 'disableDoubleEncoding': True}}}
DEBUG:botocore.hooks:Event request-created.s3.PutObject: calling handler <bound method RequestSigner.handler of <botocore.signers.RequestSigner object at 0x120bc47c0>>
DEBUG:botocore.hooks:Event choose-signer.s3.PutObject: calling handler <bound method ClientCreator._default_s3_presign_to_sigv2 of <botocore.client.ClientCreator object at 0x1203be9e0>>
DEBUG:botocore.hooks:Event choose-signer.s3.PutObject: calling handler <function set_operation_specific_signer at 0x110ea2c20>
DEBUG:botocore.hooks:Event before-sign.s3.PutObject: calling handler <function remove_arn_from_signing_path at 0x110ebce50>
DEBUG:botocore.auth:Calculating signature using v4 auth.
DEBUG:botocore.auth:CanonicalRequest:
PUT
/data/raw/amazon/2023-07-11_amazon_raw_data.csv

content-md5:YmDjjwY3eVhcUwc9vZT0hg==
host:bestpricenphu.s3.amazonaws.com
x-amz-content-sha256:UNSIGNED-PAYLOAD
x-amz-date:20230711T161253Z

content-md5;host;x-amz-content-sha256;x-amz-date
UNSIGNED-PAYLOAD
DEBUG:botocore.auth:StringToSign:
AWS4-HMAC-SHA256
20230711T161253Z
20230711/us-east-1/s3/aws4_request
8b709371a0a816e48687f83b4593f3d0f2ec0f9a15a6fc6e3a75b8a38e3a1bcc
DEBUG:botocore.auth:Signature:
600ade13cd907938c981c976db5cbd025f4b3370bdc686620bd1a2b903638a0f
DEBUG:botocore.hooks:Event request-created.s3.PutObject: calling handler <function add_retry_headers at 0x110ebcca0>
DEBUG:botocore.endpoint:Sending http request: <AWSPreparedRequest stream_output=False, method=PUT, url=https://bestpricenphu.s3.amazonaws.com/data/raw/amazon/2023-07-11_amazon_raw_data.csv, headers={'User-Agent': b'Boto3/1.28.1 md/Botocore#1.31.1 ua/2.0 os/macos#22.1.0 md/arch#x86_64 lang/python#3.10.11 md/pyimpl#CPython cfg/retry-mode#legacy Botocore/1.31.1', 'Content-MD5': b'YmDjjwY3eVhcUwc9vZT0hg==', 'Expect': b'100-continue', 'X-Amz-Date': b'20230711T161253Z', 'X-Amz-Content-SHA256': b'UNSIGNED-PAYLOAD', 'Authorization': b'AWS4-HMAC-SHA256 Credential=AKIAV6IEX2Z6QRBLWLKQ/20230711/us-east-1/s3/aws4_request, SignedHeaders=content-md5;host;x-amz-content-sha256;x-amz-date, Signature=600ade13cd907938c981c976db5cbd025f4b3370bdc686620bd1a2b903638a0f', 'amz-sdk-invocation-id': b'53265618-6e1e-4c76-937c-d72592e8b011', 'amz-sdk-request': b'attempt=1', 'Content-Length': '45850'}>
DEBUG:botocore.httpsession:Certificate path: /Users/macbook/Documents/Documents_MacBook_Pro/ISTT/AirflowTutorial/envlibs/lib/python3.10/site-packages/certifi/cacert.pem
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): bestpricenphu.s3.amazonaws.com:443
DEBUG:botocore.awsrequest:Waiting for 100 Continue response.
DEBUG:botocore.awsrequest:100 Continue response seen, now sending request body.
DEBUG:urllib3.connectionpool:https://bestpricenphu.s3.amazonaws.com:443 "PUT /data/raw/amazon/2023-07-11_amazon_raw_data.csv HTTP/1.1" 200 0
DEBUG:botocore.parsers:Response headers: {'x-amz-id-2': '8+jhg3KsMUwZ2LIgzeu2qZ6JjgNNYQxJFlUZVKE5zQV2DdloYffeUAkAUuwcTjRpB0PlPlXQKpQdV/Us6DNufg==', 'x-amz-request-id': 'JCKFMWBXA0XED6DZ', 'Date': 'Tue, 11 Jul 2023 16:12:55 GMT', 'x-amz-server-side-encryption': 'AES256', 'ETag': '"6260e38f063779585c53073dbd94f486"', 'Server': 'AmazonS3', 'Content-Length': '0'}
DEBUG:botocore.parsers:Response body:
b''
DEBUG:botocore.hooks:Event needs-retry.s3.PutObject: calling handler <botocore.retryhandler.RetryHandler object at 0x120d3a890>
DEBUG:botocore.retryhandler:No retry needed.
DEBUG:botocore.hooks:Event needs-retry.s3.PutObject: calling handler <bound method S3RegionRedirectorv2.redirect_from_error of <botocore.utils.S3RegionRedirectorv2 object at 0x120d3a950>>
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 306,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 188776,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 3.805315,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2023, 7, 11, 16, 12, 53, 158785),
 'httpcompression/response_bytes': 1210631,
 'httpcompression/response_count': 1,
 'log_count/DEBUG': 41,
 'log_count/INFO': 10,
 'log_count/WARNING': 1,
 'memusage/max': 114470912,
 'memusage/startup': 114470912,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2023, 7, 11, 16, 12, 49, 353470)}
INFO:scrapy.core.engine:Spider closed (finished)
